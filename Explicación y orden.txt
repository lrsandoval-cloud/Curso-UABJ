1) listado.py: localiza las URLs de las páginas de resultados que guardamos
en la carpeta "listados" y genera un archivo TXT con los que corresponden
a cada medio.

2) scraping.py: deben configurarse el sitio (línea 7) y los parámetros (tags y
atributos, líneas 9 a 27) correspondientes al sitio web específico. Captura los contenidos
de los artículos listados y genera un pickle con el dataframe
correspondiente a cada uno en la carpeta 'pickles' (si no existe esta
carpeta, crearla primero)

3) revisa.py: debe configurarse la lista inicial (línea 4) con los nombres
de los medios. Une todos los pickles en uno que se llama "base.pkl" y crea
un archivo "listado.html" con la lista de todos los artículos (id y
título). Este archivo debe abrirse en un browser para detectar los
registros que deban eliminarse. Tomar nota de los ids.

4) depura.py: depura.py: script interactivo. Primero pide los IDs de los
registros a eliminar y después va uno por uno pidiendo confirmación.
Al final reindexa la base y copia el pickle actualizado con el mismo
nombre (base.pkl)

5) lemas.py: además de la librería se necesita descargar el modelo. Desde la
terminal, con la instrucción:

     python -m spacy download es_core_news_sm

   El script actualiza el pickle base.pkl y le agrega una columna con copetes + textos
   de cada nota lematizados

6) palabras.py: cuenta la frecuencia de cada palabra en los textos lematizados y arma
nubes de tags para cada medio

7) caracteres.py: hace un gráfico con la cantidad de caracteres promedio según medio

8) frecuencias.py: a partir de configurar una keyword genera un gráfico de barras
con la frecuencia de uso de la palabra entre los distintos medios. Debe indicarse la
keyword

9) correlaciones.py: compara la similitud entre las palabras (lematizadas) que se
utilizan, de a pares, entre medio y medio. Devuelve tres resultados:
a) tabla de correlaciones: valor que va de 0 (totalmente disímiles) a 1
(completamente idénticos)
b) gráfico de puntos visualizando la correspondencia entre dos medios (indicar al
comienzo del script cuáles deben compararse)
c) tabla de palabras coincidentes entre medio y medio

10) prepara_sentimientos.py. Asigna los valores de análisis de sentimiento a cada
oración y crea un data frame con todas las oraciones. Este script necesita algunas
librerías que PyCharm no detecta automáticamente para su instalación.
Desde la terminal (uno de los botones de la barra inferior izquierda) indicar:

pip install transformers
pip install torch

(y Enter luego de cada instrucción). También requiere instalar una librería de
nltk, algo que se hace descomentando las líneas 4 y 5 (luego se pueden comentar
de nuevo)

11) sentimientos.py: Este script toma los datos del dataframe recién creado y
los devuelve de diferentes formas:
- tabla de datos generales
- tabla de valores medios según distintos niveles de confianza
- tabla de valores medios según medio
- gráficos de evolución en tiempo para el total y discriminado por medio

12) codo.py: realiza un gráfico de codo (Elbow method) para determinar el
número óptimo de grupos

13) kmeans.py: una vez definida la cantidad de clústers, indicarlas en la línea 1
de este script. En la línea 2 se puede modificar la cantidad de artículos
emblemáticos que nos mostrará para describir las características de cada grupo.
Genera el archivo 'cluster.html' que contiene 20 palabras clave y una lista de
titulos y enlaces a los artículos más destacados de cada grupo.

14) nube_clusters.py: genera un archivo con una nube de puntos que ilustra la
distribución de los puntos asignados a cada grupo

15) cuotas.py: este script sirve para crear una muestra representativa con fines
de realizar análisis de contenido. Se debe indicar en la primer línea la cantidad
de artículos con que contará la muestra. Las cuotas son representativas por medio
y clúster, y los artículos se selecionan entre los más emblemáticos en cada caso
(es decir los que tienen vectores más cercanos al centroide del clúster respectivo).

16) k_sentir.py: genera dos gráficos. El primero con el promedio de label para cada
clúster y el segundo con la evolución de esos promedios en el tiempo.

